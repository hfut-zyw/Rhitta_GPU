{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbae3528",
   "metadata": {},
   "source": [
    "# 案例5：使用BiLSTM对IMDB做分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84a0e95",
   "metadata": {},
   "source": [
    "由于本框架计算太慢，句子长度不能设置太大，实际上大多数句子长度   \n",
    "都超过200了，仅仅依靠前面几个词根本无法预测，这里仅仅是展示模型是否跑的通  \n",
    "后面的解释可能会按256个词来解释\n",
    "\n",
    "本案例为了能跑通，更改了以下几个参数：   \n",
    "1.max_seq_len=256 --> 8  \n",
    "2.vocab_lens=len(loader.dict) --> 20 (这样单词id会溢出词典，注意）  \n",
    "3.训练的样本数量：N= len(seqs) --> 12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562aab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\Rhitta_GPU\")\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import rhitta.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21e74e",
   "metadata": {},
   "source": [
    "### 第一步：载入数据集 \n",
    "会获得两个列表，分别是句子和标签  \n",
    "IMDBLoader()接口接收一个指定句子长度的参数max_seq_len  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edad1bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 8 # 设置每个句子的长度\n",
    "loader=nn.IMDBLoader(max_seq_len=max_seq_len)\n",
    "seqs,labels=loader.load(r\"D:\\Rhitta_GPU\\data\\dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee1d5313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[552, 35, 14, 554, 171, 188, 34, 2] 1\n",
      "[16, 5344, 946, 5, 2, 10059, 33, 3898] 1\n",
      "[16, 21, 529, 19, 4, 3179, 4752, 5200] 0\n",
      "[41, 218, 30, 5201, 12, 162, 37, 5812] 0\n",
      "[25376, 4556, 3, 44405, 12118, 13, 2284, 37792] 1\n",
      "[10, 6192, 123, 176, 83, 105, 14, 7] 1\n",
      "[49, 18, 97, 29, 77, 54, 50, 22] 0\n",
      "[10, 301, 14, 398, 34, 4, 464, 13] 0\n",
      "[112, 464, 5, 1966, 1249, 14, 21, 20] 0\n",
      "[8, 8, 49, 18, 7, 407, 5, 2124] 1\n"
     ]
    }
   ],
   "source": [
    "## test \n",
    "for i in range(10):\n",
    "    print(seqs[i],labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d385bebd",
   "metadata": {},
   "source": [
    "### 第二步：构造模型 \n",
    "先把句子的256个词，也就是256个数字丢进embedding层，变成256个向量，  \n",
    "再把256个向量丢进BiLSTM中，获得一个输出，最后送入分类头  \n",
    "注意：BiLSTM最后的汇聚层需要忽略掉pad过来的向量,  \n",
    "由于实现起来有些繁琐，这里就不忽略了，影响不大  \n",
    "不忽略相当于后面的神经元用于传递之前的信息，没有新信息加入 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5dff3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class zyw(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(zyw,self).__init__()        \n",
    "        self.bilstm=nn.BiLSTM(input_size=6,hidden_size=4,time_dimension=max_seq_len,mode=1)\n",
    "        self.linear=nn.Linear(8,1,activation = \"Logistic\") # 注意BiLSTM把隐藏层拼接了，向量维度变成2倍了\n",
    "    def __call__(self,seq_embeddings,h_0,c_0,h_1,c_1):\n",
    "        x=self.bilstm(seq_embeddings,h_0,c_0,h_1,c_1)\n",
    "        x=self.linear(x)\n",
    "        return x\n",
    "vocab_lens=len(loader.dict)\n",
    "vocab_lens=20 # 字典太大训练不动，但是取词的时候，词的id很容易超过这个数\n",
    "embedding=nn.Embedding(numembeddings=vocab_lens, embeddingdim=6, paddingidx=0)\n",
    "model = zyw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c5393",
   "metadata": {},
   "source": [
    "#### 第三步：构造计算图  \n",
    "坑节点包括： \n",
    "\n",
    "句子列表：必须是一个固定不动的对象，后面需要往里面填写每个句子的数字  \n",
    "embedding一旦实例化，就不能变动，只能改输入对象的内部数值\n",
    "\n",
    "初始隐藏状态节点：由于是双向LSTM，需要4个，形状(1,4)  \n",
    "标签节点：由于是二分类，形状为(1,1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e36e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造坑位，注意，叶子节点不是输入的列表，而是编码器里面的词典，已经自动创建好了\n",
    "# 当词典更新set_value时，所有下游节点全部reset_value\n",
    "seq = [i for i in range(max_seq_len)]\n",
    "h_0,c_0,h_1,c_1=nn.to_tensor((1,4)),nn.to_tensor((1,4)),nn.to_tensor((1,4)),nn.to_tensor((1,4))\n",
    "label = nn.to_tensor((1,1))\n",
    "\n",
    "# 构造计算图\n",
    "seq_embedding = embedding(seq)\n",
    "output = model(seq_embedding,h_0,c_0,h_1,c_1)\n",
    "loss = nn.BinaryClassLoss(output,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86a90f",
   "metadata": {},
   "source": [
    "### 第四步：初始化优化器  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08eef19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = nn.Adam(nn.default_graph, loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377f5ef",
   "metadata": {},
   "source": [
    "### 第五步：开始训练  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97708c57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "更新前的随机词典：\n",
      "[[ 0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.04601878  0.00404048  0.04015696 -0.06519445 -0.03926611  0.0139227 ]\n",
      " [-0.06951123  0.08241394  0.02143262  0.06112293  0.01719459 -0.03234663]\n",
      " [-0.00495689 -0.00952639 -0.08017523 -0.0049339   0.06948158 -0.01158497]\n",
      " [-0.05801717 -0.04864316 -0.01222767  0.09520323 -0.0561946   0.0125205 ]\n",
      " [-0.09940845  0.03058567 -0.06553359 -0.09607915 -0.03470116 -0.07746006]\n",
      " [ 0.02574757  0.08103257 -0.00861659 -0.08763861  0.00053078  0.06172405]\n",
      " [-0.01261335 -0.01195699 -0.00790909  0.06745773  0.01438359 -0.0432668 ]\n",
      " [-0.07392157  0.00809994  0.08484132  0.08505075 -0.01008637  0.06179531]\n",
      " [ 0.0030789   0.0487923   0.02765147 -0.02074502  0.0231011   0.09824966]\n",
      " [-0.03578965 -0.08761168  0.02594208 -0.01307079  0.06163082 -0.01358586]\n",
      " [ 0.05578862  0.02245893  0.092424   -0.05362544 -0.00972471 -0.04428356]\n",
      " [ 0.07484926 -0.02366823 -0.09911224  0.04430081 -0.00841923 -0.00920663]\n",
      " [ 0.02676214  0.06327197  0.08703353  0.06904646 -0.03202954  0.03444296]\n",
      " [-0.01285214  0.07879407  0.04201142  0.00959265  0.06963184  0.01009394]\n",
      " [-0.08374249 -0.05770571 -0.02031106 -0.01629067  0.05843795 -0.07895623]\n",
      " [-0.04759348 -0.08748662  0.00497911  0.00683976 -0.0933935  -0.0852092 ]\n",
      " [-0.02791182 -0.06868168 -0.05379565 -0.00373487 -0.04043877  0.0174353 ]\n",
      " [-0.08165128 -0.03259038 -0.09113629 -0.08790202  0.00799482 -0.04439018]\n",
      " [ 0.0215529   0.01567511 -0.06697443  0.02731935 -0.03042926  0.07366363]]\n",
      "epoch:1 , average_loss:0.9893749000353604\n",
      "epoch:2 , average_loss:0.8663498296578028\n",
      "epoch:3 , average_loss:0.8411084832551297\n",
      "更新后的词典：\n",
      "[[-0.05649016 -0.04927701 -0.04806061  0.15907731  0.00178527  0.05822795]\n",
      " [-0.00156807 -0.06583477  0.15503936  0.04629727 -0.00317504  0.07930903]\n",
      " [-0.14579108  0.02817459  0.06145587  0.22798977  0.09353109  0.02435572]\n",
      " [-0.01749295 -0.10734621 -0.14469249 -0.04212084  0.12125494  0.05556165]\n",
      " [-0.01051013 -0.11818024 -0.0661573  -0.00720834  0.01919932  0.04678992]\n",
      " [-0.06861676 -0.06855189 -0.10726222 -0.27821925  0.01145983 -0.01735449]\n",
      " [-0.1242651   0.13774998 -0.00981059 -0.02559029  0.03498846  0.07474569]\n",
      " [-0.07225565 -0.00482027  0.07053885  0.14182547  0.08221801  0.02598057]\n",
      " [-0.07392157  0.00809994  0.08484132  0.08505075 -0.01008637  0.06179531]\n",
      " [ 0.0030789   0.0487923   0.02765147 -0.02074502  0.0231011   0.09824966]\n",
      " [-0.03578965 -0.08761168  0.02594208 -0.01307079  0.06163082 -0.01358586]\n",
      " [ 0.05578862  0.02245893  0.092424   -0.05362544 -0.00972471 -0.04428356]\n",
      " [ 0.07484926 -0.02366823 -0.09911224  0.04430081 -0.00841923 -0.00920663]\n",
      " [ 0.02676214  0.06327197  0.08703353  0.06904646 -0.03202954  0.03444296]\n",
      " [-0.01285214  0.07879407  0.04201142  0.00959265  0.06963184  0.01009394]\n",
      " [-0.08374249 -0.05770571 -0.02031106 -0.01629067  0.05843795 -0.07895623]\n",
      " [-0.04759348 -0.08748662  0.00497911  0.00683976 -0.0933935  -0.0852092 ]\n",
      " [-0.02791182 -0.06868168 -0.05379565 -0.00373487 -0.04043877  0.0174353 ]\n",
      " [-0.08165128 -0.03259038 -0.09113629 -0.08790202  0.00799482 -0.04439018]\n",
      " [ 0.0215529   0.01567511 -0.06697443  0.02731935 -0.03042926  0.07366363]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2 # 因为只拿12个句子，这里batch_size如果取16，模型就不更新了\n",
    "epochs = 3\n",
    "print(\"更新前的随机词典：\")\n",
    "print(embedding.vocab.value)\n",
    "for epoch in range(epochs):\n",
    "    count = 0\n",
    "    N= len(seqs)\n",
    "    N = 12 # 就拿前10条句子跑吧，否则还是跑不动\n",
    "\n",
    "    # 填坑并训练\n",
    "    for i in range(N):\n",
    "        # 句子的列表对象填坑\n",
    "        for j in range(max_seq_len):\n",
    "            if seqs[i][j] < vocab_lens :\n",
    "                seq[j]=seqs[i][j]\n",
    "            else:\n",
    "                seq[j]=0\n",
    "        # 输入隐藏状态\n",
    "        h_0.set_value(cp.zeros((1, 4)))\n",
    "        c_0.set_value(cp.zeros((1, 4)))\n",
    "        h_1.set_value(cp.zeros((1, 4)))\n",
    "        c_1.set_value(cp.zeros((1, 4)))\n",
    "        # 输入标签\n",
    "        label.set_value(labels[i])\n",
    "        # 前向反向传播\n",
    "        optimizer.one_step() \n",
    "        # 更新计数器\n",
    "        count += 1\n",
    "        # 计数器达到batch_size就更新模型参数\n",
    "        if count >= batch_size: \n",
    "            optimizer.update() \n",
    "            count = 0\n",
    "\n",
    "    # 每个epoch后评估模型的平均平方损失\n",
    "    acc_loss = 0\n",
    "    for i in range(N):\n",
    "        for j in range(max_seq_len):\n",
    "            if seqs[i][j] < vocab_lens :\n",
    "                seq[j]=seqs[i][j]\n",
    "            else:\n",
    "                seq[j]=0\n",
    "        h_0.set_value(cp.zeros((1, 4)))\n",
    "        c_0.set_value(cp.zeros((1, 4)))\n",
    "        h_1.set_value(cp.zeros((1, 4)))\n",
    "        c_1.set_value(cp.zeros((1, 4)))\n",
    "        label.set_value(labels[i])\n",
    "        loss.forward()\n",
    "        acc_loss += loss.value\n",
    "    average_loss = acc_loss / N\n",
    "    print(\"epoch:{} , average_loss:{}\".format(epoch+1, cp.sqrt(average_loss)[0][0]))\n",
    "print(\"更新后的词典：\")\n",
    "print(embedding.vocab.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynet38",
   "language": "python",
   "name": "mynet38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
